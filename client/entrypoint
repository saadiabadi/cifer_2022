#!./.mnist-keras/bin/python
import json
import os

import docker
import fire
import numpy as np

from __future__ import print_function
import sys
import yaml
import os
import pickle
import tensorflow as tf
from ttictoc import tic,toc
import threading
import psutil
from datetime import datetime
import keras
import pickle
import json
import numpy as np
from sklearn import metrics
import yaml

import tensorflow
from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Activation
from tensorflow.keras.layers import Dense, BatchNormalization, Flatten, Input
from tensorflow.keras.models import Sequential
from tensorflow.keras import activations
import random

from sklearn.model_selection import train_test_split
tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)

from fedn.utils.kerashelper import KerasHelper

#################################################################################
#################### Resources monitoring

def get_cpu_usage_pct():
    """
    Obtains the system's average CPU load as measured over a period of 500 milliseconds.
    :returns: System CPU load as a percentage.
    :rtype: float
    """
    return psutil.cpu_percent(interval=0.1)

def get_cpu_frequency():
    """
    Obtains the real-time value of the current CPU frequency.
    :returns: Current CPU frequency in MHz.
    :rtype: int
    """
    return int(psutil.cpu_freq().current)

def get_ram_usage():
    """
    Obtains the absolute number of RAM bytes currently in use by the system.
    :returns: System RAM usage in bytes.
    :rtype: int
    """
    return int(psutil.virtual_memory().total - psutil.virtual_memory().available)

def get_ram_total():
    """
    Obtains the total amount of RAM in bytes available to the system.
    :returns: Total system RAM in bytes.
    :rtype: int
    """
    return int(psutil.virtual_memory().total)

def get_ram_usage_pct():
    """
    Obtains the system's current RAM usage.
    :returns: System RAM usage as a percentage.
    :rtype: float
    """
    return psutil.virtual_memory().percent

def ps_util_monitor(round):
    global running
    running = True
    cpu_P = []
    cpu_f = []
    memo_u = []
    memo_T = []
    memo_P = []
    time_ = []
    report = {}
    # start loop
    while running:
        cpu_P.append(get_cpu_usage_pct())
        cpu_f.append(get_cpu_frequency())
        memo_u.append(int(get_ram_usage() / 1024 / 1024))
        memo_T.append(int(get_ram_total() / 1024 / 1024))
        memo_P.append(get_ram_usage_pct())

    report['round'] = round
    report['cpu_p'] = cpu_P
    report['cpu_f'] = cpu_f
    report['memory_u'] = memo_u
    report['memory_t'] = memo_T
    report['memory_p'] = memo_P

    with open('/app/resources.txt', '+a') as f:
        print(report, file=f)

def start_monitor(round):
    global t
    # create thread and start it
    t = threading.Thread(target=ps_util_monitor, args=[round])
    t.start()


def stop_monitor():
    global running
    global t
    # use `running` to stop loop in thread so thread will end
    running = False
    # wait for thread's end
    t.join()


#################################################################################


# def _get_data_path():
#     # Figure out FEDn client number from container name
#     client = docker.from_env()
#     container = client.containers.get(os.environ['HOSTNAME'])
#     number = container.name[-1]
#
#     # Return data path
#     return f"/var/data/clients/{number}/mnist.npz"

cfg = {
    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],
    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],
}


trainable_layers = {
    'VGG11': [0, 2, 4, 5, 7, 8, 10, 11],
    'VGG13': [0, 1, 3, 4, 6, 7, 9, 10, 11, 12],
    'VGG16': [0, 1, 3, 4,  6, 7, 8,  10, 11, 12, 14, 15, 16],
    'VGG19': [0, 1,  3, 4,  6, 7, 8, 9,  11, 12, 13, 14,  16, 17, 18, 19],
}

def _compile_model(input_shape=(32,32,3), dimension='VGG16', trainedLayers=0):

    num_classes = 10
    lay_count = 0

    if trainedLayers > 0:

        randomlist = random.sample(trainable_layers[dimension], trainedLayers)
        print(randomlist)

        with open('/app/layers.txt', '+a') as f:
            print(randomlist, file=f)

        model = Sequential()
        model.add(tensorflow.keras.Input(shape=input_shape))
        for x in cfg[dimension]:
            if x == 'M':
                model.add(MaxPooling2D(pool_size=(2, 2)))
            else:
                if lay_count in randomlist:
                    model.add(Conv2D(x, (3, 3), padding='same', trainable=True))
                    model.add(BatchNormalization(trainable=True))
                    model.add(Activation(activations.relu))
                else:
                    model.add(Conv2D(x, (3, 3), padding='same', trainable=False))
                    model.add(BatchNormalization(trainable=False))
                    model.add(Activation(activations.relu))
            lay_count += 1

        #model.add(Flatten())
        model.add(AveragePooling2D(pool_size=(1, 1)))
        model.add(Flatten())
        model.add(Dense(num_classes, activation='softmax'))
        opt = tensorflow.keras.optimizers.Adam(learning_rate=0.001)
        model.compile(loss='categorical_crossentropy',
                      optimizer=opt, metrics=['accuracy'])

        print(" --------------------------------------- ")
        print(" ------------------Partial MODEL CREATED------------------ ")
        print(" --------------------------------------- ")

    else:
        model = Sequential()
        model.add(tensorflow.keras.Input(shape=input_shape))
        for x in cfg[dimension]:
            if x == 'M':
                model.add(MaxPooling2D(pool_size=(2, 2)))
            else:
                print("trani: ", x)
                model.add(Conv2D(x, (3, 3), padding='same', trainable=True))
                model.add(BatchNormalization(trainable=True))
                model.add(Activation(activations.relu))

        # model.add(Flatten())
        model.add(AveragePooling2D(pool_size=(1, 1)))
        model.add(Flatten())
        model.add(Dense(num_classes, activation='softmax'))
        opt = tensorflow.keras.optimizers.Adam(learning_rate=0.001)
        model.compile(loss='categorical_crossentropy',
                      optimizer=opt, metrics=['accuracy'])
        print(" --------------------------------------- ")
        print(" ------------------Full MODEL CREATED------------------ ")
        print(" --------------------------------------- ")

    return model


def _load_data(data_path='../data', is_train=True):
    # Load data
    # if data_path is None:
    #     data = np.load(_get_data_path())
    # else:
    #     data = np.load(data_path)

    if is_train:
        with open(os.path.join(data_path, 'trainx.pyp'), 'rb') as fh:
            X = pickle.loads(fh.read())
        with open(os.path.join(data_path, 'trainy.pyp'), 'rb') as fh:
            y = pickle.loads(fh.read())
    else:
        # Test error (Client has a small dataset set aside for validation)
        # try:
        with open(os.path.join(data_path, 'testx.pyp'), 'rb') as fh:
            X = pickle.loads(fh.read())
        with open(os.path.join(data_path, 'testy.pyp'), 'rb') as fh:
            y = pickle.loads(fh.read())
        print("X shape: : ", X.shape)

    # Normalize
    # X = X.astype('float32')
    # X = np.expand_dims(X, -1)
    # X = X / 255
    # y = tf.keras.utils.to_categorical(y, NUM_CLASSES)

    return X, y


def init_seed(out_path='seed.npz'):
    weights = _compile_model(dimension='VGG16').get_weights()
    helper = KerasHelper()
    helper.save_model(weights, out_path)


def train(in_model_path, out_model_path, data_path='../data', settings):
    # Load data
    x_train, y_train = _load_data(data_path)
    global round
    round = 1
    print("-- RUNNING TRAINING --", flush=True)

    # Load model
    model = _compile_model(dimension='VGG16')
    helper = KerasHelper()
    weights = helper.load_model(in_model_path)
    model.set_weights(weights)

    start_monitor(round)
    tic()
    # Train
    model.fit(x_train, y_train, batch_size=settings['batch_size'], epochs=settings['epochs'], verbose=True)

    elapsed = toc()

    stop_monitor()
    round += 1

    with open('/app/time.txt', '+a') as f:
        print(elapsed, file=f)

    # Save
    weights = model.get_weights()
    helper.save_model(weights, out_model_path)


def validate(in_model_path, out_json_path, data_path='../data'):
    # Load data
    # x_train, y_train = _load_data(data_path)
    x_test, y_test = _load_data(data_path, is_train=False)

    print("-- RUNNING VALIDATION --", flush=True)


    # Load model
    model = _compile_model(dimension='VGG16')
    helper = KerasHelper()
    weights = helper.load_model(in_model_path)
    model.set_weights(weights)

    # Evaluate
    model_score_test = model.evaluate(x_test, y_test, verbose=0)
    print('Test loss:', model_score_test[0])
    print('Test accuracy:', model_score_test[1])
    y_pred = model.predict(x_test)
    y_pred = np.argmax(y_pred, axis=1)
    clf_report = metrics.classification_report(y_test.argmax(axis=-1), y_pred)

    print(clf_report)



    # JSON schema
    report = {
        "classification_report": clf_report,
        "loss": model_score_test[0],
        "accuracy": model_score_test[1],

    }
    print("-- VALIDATION COMPLETE! --", flush=True)
    # Save JSON
    with open(out_json_path, "w") as fh:
        fh.write(json.dumps(report))


if __name__ == '__main__':
    with open('settings.yaml', 'r') as fh:
        try:
            settings = dict(yaml.safe_load(fh))
        except yaml.YAMLError as e:
            raise(e)

    fire.Fire({
        'init_seed': init_seed,
        'train': train,
        'validate': validate,
        # '_get_data_path': _get_data_path,  # for testing
    })
